{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bf540cf",
   "metadata": {},
   "source": [
    "# This notebook demonstrates the process of:\n",
    "- Accessing heat risk data from your own AWS S3 bucket \n",
    "- Reading it into pandas dataframe\n",
    "- Merging with geospatial auxiliary data on local machine\n",
    "- Data manipulation \n",
    "- Visualization data on a map\n",
    "\n",
    "## There are two tasks for this example:\n",
    "1. What is the heat risk distribution in California at tract level?\n",
    "2. What is the heat risk distribution in United States at the state level?\n",
    "\n",
    "## Prerequisites\n",
    "This notebook assumes the following:\n",
    "- Understanding of the general idea of the model and methodology behind the datasets: https://firststreet.org/research-library/heat-model-methodology\n",
    "    - The FSF-EHM utilizes several existing methods from the heat science community combined with scalable computational techniques and satellite imagery to produce new high-resolution heat hazards across the contiguous United States (CONUS). U.S. Federal Open Data sources support the production of a high resolution extreme heat product that allows individuals, communities, businesses, and governments to better understand and prepare for their heat risks both today and 30 years into the future.\n",
    "\n",
    "\n",
    "- Have an established AWS account that allows for the creation of an access key and has access to the First Street data.  This process is described in the ```How to access data on AWS.docx``` document.  Follow the directions for processing the data via the Amazon S3 Buckets.\n",
    "\n",
    "Note: For the purposes of this this notebook, you may copy and paste the values from the provided `credentials.json` file instead of using your own AWS account credentials.\n",
    "\n",
    "- Obtain the geospatial auxiliary data (https://catalog.data.gov/dataset/tiger-line-shapefile-2022-state-california-ca-census-tract) and downloaded to your local machine.\n",
    "    The data should be in the directory `..\\\\Climate Risk data\\\\Auxiliary Data\\\\`. \n",
    "        If this is not the location of your data, modify the `auxilliaryDataDirectory` variable below to reflect the appropriate directory.\n",
    "    \n",
    "- Packages installed:\n",
    "    * pandas\n",
    "    * getpass\n",
    "    * geopandas\n",
    "    * matplotlib\n",
    "    * folium\n",
    "    * mapclassify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77b230",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 - What is the heat risk distribution in California at the tract level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45703ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user for their secret key, access key, and URI for the S3 bucket\n",
    "# Use getpass package to hide credentials\n",
    "import getpass\n",
    "\n",
    "tract_S3_URI = getpass.getpass('Enter tract S3 URI:') # this can be from the credentials file (tract_S3_URI)\n",
    "key = getpass.getpass('Enter your aws_access_key_id:') # this can be from the credentials file\n",
    "secret = getpass.getpass('Enter your aws_secret_access_key:') # this can be from the credentials file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read heat risk tract data from S3 bucket into pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "heatRiskTractData = pd.read_csv(\n",
    "    tract_S3_URI, # S3 object URI\n",
    "    storage_options={\n",
    "        \"key\": key, # aws_access_key_id\n",
    "        \"secret\": secret # aws_secret_access_key\n",
    "    })\n",
    "\n",
    "heatRiskTractData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of our auxiliary data\n",
    "auxilliaryDataDirectory = \"..\\\\Climate Risk data\\\\Auxiliary Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tract geospatial data for California and rename the column\n",
    "import geopandas as gpd\n",
    "\n",
    "auxilliaryData_CA = gpd.read_file(f\"{auxilliaryDataDirectory}tl_2022_06_tract.shp\")\n",
    "auxilliaryData_CA = auxilliaryData_CA.rename(columns = {'GEOID':'fips'}) # used to match the same column name 'fips' for merging purposes later\n",
    "auxilliaryData_CA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust fips format and merge the auxilliary and the heat risk datasets.\n",
    "# Use the FIPS code as the common key\n",
    "heatRiskTractData.fips = heatRiskTractData['fips'].astype(str).str.zfill(11)\n",
    "mergedHeatRiskTractData = pd.merge(heatRiskTractData, auxilliaryData_CA, on = 'fips')\n",
    "mergedHeatRiskTractData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefcf2b",
   "metadata": {},
   "source": [
    "The original dataset quantifies the climate risk by assigning each properties within the geographical unit to one risk factor from 1 - 10, with 10 the most severe. In order to simplify the calculation, we create a new variable to derive the weighted risk by adding up the number of properties times factor index, then divided by the total counts of the properties to get the average risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the weighted average risk and store it in the dataframe\n",
    "mergedHeatRiskTractData['average_risk'] = 0\n",
    "for i in range (1,11):\n",
    "    mergedHeatRiskTractData['average_risk'] += mergedHeatRiskTractData[f'count_heatfactor{i}'] * i\n",
    "mergedHeatRiskTractData['average_risk'] /= mergedHeatRiskTractData['count_property']\n",
    "\n",
    "# Subset with the attributes needed\n",
    "final_data = mergedHeatRiskTractData[['fips', 'average_risk', 'geometry']]\n",
    "\n",
    "# Display the final data\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a georeferenced dataframe\n",
    "crs = {'init':'EPSG:4326'} # EPSG:4326 is a popular standard coordinate system \n",
    "georeferencedData = gpd.GeoDataFrame(final_data, crs = crs, geometry = final_data.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "georeferencedData.plot(column = 'average_risk', cmap = 'OrRd',\n",
    "                       legend = True, legend_kwds={'shrink': 0.5, 'label':'Risk'},\n",
    "                       markersize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210795e",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 - What is the heat risk distribution in United States at states level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6160cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user for the URI for the state's S3 data bucket\n",
    "# Note: This can be from the credentials file (state_S3_URI)\n",
    "state_S3_URI = getpass.getpass('Enter state_S3_URI:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71392f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read heat risk state data from S3 bucket into pandas dataframe\n",
    "USHeatRiskData = pd.read_csv(\n",
    "    state_S3_URI, #S3 object URI\n",
    "    storage_options={\n",
    "        \"key\": key, # aws_access_key_id\n",
    "        \"secret\": secret # aws_secret_access_key\n",
    "    })\n",
    "\n",
    "# Display the first few rows of the state data\n",
    "USHeatRiskData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4fac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the geopandas library to deal with GIS shape data\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read data that has the geospatial information for sub-county areas\n",
    "auxilliaryData_US = gpd.read_file(f\"{auxilliaryDataDirectory}cb_2018_us_state_500k.shp\")\n",
    "auxilliaryData_US = auxilliaryData_US.to_crs(\"EPSG:4326\")\n",
    "auxilliaryData_US = auxilliaryData_US.rename(columns = {'GEOID':'fips'})\n",
    "\n",
    "# Display the first few rows of the geospatial data\n",
    "auxilliaryData_US.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2840c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted average risk for each sub-county area in WV\n",
    "USHeatRiskData['average_risk'] = 0\n",
    "for i in range (1,11):\n",
    "    USHeatRiskData['average_risk'] += USHeatRiskData[f'count_heatfactor{i}'] * i\n",
    "USHeatRiskData['average_risk'] /= USHeatRiskData['count_property']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge both datasets with the same column \"fips\" and extract the columns we are interested in\n",
    "USHeatRiskData.fips = USHeatRiskData['fips'].astype(str).str.zfill(2)\n",
    "result = pd.merge(USHeatRiskData, auxilliaryData_US, on = 'fips')\n",
    "final_data = result[['fips', 'name', 'average_risk', 'geometry']]\n",
    "\n",
    "# Display the first few rows of the final data\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the geodataframe\n",
    "crs = {'init':'EPSG:4326'}\n",
    "georeferencedData = gpd.GeoDataFrame(final_data, crs = crs, geometry = final_data.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68913f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive map to explore the data\n",
    "georeferencedData.explore(\n",
    "    column = \"average_risk\",  # make choropleth based on \"average_risk\" column\n",
    "    tooltip = \"name\",  # show \"name\" value in tooltip (on hover)\n",
    "    popup = True,  # show all values in popup (on click)\n",
    "    tiles = \"CartoDB positron\",  # use the \"CartoDB positron\" style tiles\n",
    "    cmap = \"OrRd\",  # use \"OrRd\" matplotlib colormap\n",
    "    style_kwds = dict(color = \"black\"),# use black outline\n",
    "    legend_kwds = dict(caption = \"Heat Risk\") # rename legend\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a9411",
   "metadata": {},
   "source": [
    "## This concludes this example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
